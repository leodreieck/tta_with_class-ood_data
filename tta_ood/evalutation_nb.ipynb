{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import glob\n",
    "from eval_utils import get_output_files, generate_results_df_from_output_files, transform_name_to_label, generate_thresholding_df_from_output_files\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing\n",
    "accumulate output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corruptions_standard = [\n",
    "    'gaussian_noise', 'impulse_noise', 'shot_noise', 'defocus_blur', 'glass_blur', 'zoom_blur', 'motion_blur',\n",
    "    'snow', 'frost', 'brightness', 'elastic_transform', 'jpeg_compression', 'contrast', 'pixelate', 'fog'   \n",
    "]\n",
    "\n",
    "corruptions_holdout = ['gaussian_blur', 'saturate', 'speckle_noise', 'spatter']\n",
    "\n",
    "severities = [1,2,3,4,5]\n",
    "\n",
    "methods = ['source', 'tent', 'norm', 'gce']\n",
    "\n",
    "n_ood_samples = [100,200]\n",
    "\n",
    "ood_datasets = [\"CIFAR100\", \"CIFAR100C\", \"SVHN\", \"SVHNC\"]\n",
    "\n",
    "ood_settings = list()\n",
    "for dataset in ood_datasets:\n",
    "    for number in n_ood_samples:\n",
    "        ood_settings.append(dataset + \"_\" + str(number))\n",
    "ood_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "files.extend(get_output_files(output_dir=\"./04_output/output_leo/31*/holdout/\"))\n",
    "#files = get_output_files(output_dir=\"./04_output/output_leo/01_verify_evis_results/*_final/\")\n",
    "#files.extend(get_output_files(output_dir=\"./04_output/output_leo/02*/\"))\n",
    "#files.extend(get_output_files(output_dir=\"./04_output/output_leo/04*/\"))\n",
    "#files.extend(get_output_files(output_dir=\"./04_output/output_leo/05*/\"))\n",
    "#files.extend(get_output_files(output_dir=\"./04_output/output_leo/06*/\"))\n",
    "#files.extend(get_output_files(output_dir=\"./04_output/output_leo/08*/\"))\n",
    "files.extend(get_output_files(output_dir=\"./04_output/output_leo/08*/*/\"))\n",
    "files.extend(get_output_files(output_dir=\"./04_output/output_leo/09*/*/\"))\n",
    "files.extend(get_output_files(output_dir=\"./04_output/output_leo/10*/*/\"))\n",
    "#files.extend(get_output_files(output_dir=\"./04_output/output_leo/10*/*/\"))\n",
    "\n",
    "print((len(files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = generate_results_df_from_output_files(files)\n",
    "results_df[results_df[\"Adaptation\"]==\"norm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"nOOD\"] = results_df[[\"CIFAR100\", \"CIFAR100C\", \"SVHN\", \"SVHNC\"]].sum(axis=1)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agg results\n",
    "transform them into human readable df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results = pd.pivot_table(results_df, values=[\"Error\"], index=['Adaptation', 'LearningRate', \"PL_Threshold\", \"OOD_DetMethod\", \"OOD_Threshold\", 'CIFAR100', 'CIFAR100C', 'SVHN', 'SVHNC', 'CorruptionType','nOOD'], columns=[\"Epoch\"], aggfunc={np.mean, \"count\"})[\"Error\"]\n",
    "agg_results = agg_results.rename(columns={\"mean\": \"ErrorRate\"}) \n",
    "agg_results.ErrorRate = agg_results.ErrorRate.round(decimals=1)\n",
    "\n",
    "pd.set_option('display.max_rows', 195)\n",
    "agg_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table1: All Methods, All Learning Rates, All nOODSamples, No oodDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noods = [0,100,200]\n",
    "filtered_df_1 = agg_results.query(\"nOOD in @noods and PL_Threshold not in ['0.5','0.7'] and OOD_DetMethod == 'none'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_1[(\"ErrorRate\", \"min\")] = filtered_df_1[[(\"ErrorRate\", str(x))for x in range(1,7)]].min(axis=1)\n",
    "filtered_df_1[(\"ErrorRate\", \"minEpoch\")] = filtered_df_1[[(\"ErrorRate\", str(x))for x in range(1,7)]].idxmin(axis=1)\n",
    "filtered_df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_results = dict()\n",
    "for row in filtered_df_1[(\"ErrorRate\", \"min\")].index:\n",
    "    new_key = row[:5]\n",
    "    if new_key not in dict_of_results.keys():\n",
    "        dict_of_results[new_key] = dict()\n",
    "    used = False\n",
    "    for i in range(5,9):\n",
    "        if row[i] != 0:\n",
    "            dict_of_results[new_key][filtered_df_1.index.names[i]+\"-\"+str(row[i])] = filtered_df_1[(\"ErrorRate\", \"min\")].loc[row]\n",
    "            used=True\n",
    "    if used==False:\n",
    "        dict_of_results[new_key][\"noOOD\"] = filtered_df_1[(\"ErrorRate\", \"min\")].loc[row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_1 = pd.DataFrame(dict_of_results).T\n",
    "df_1_1.index.names = [\"Adaptation\",\"lr\",\"PL_Threshold\",\"oodMethod\",\"oodThreshold\"]\n",
    "reorderlist = [\"source\", \"norm\",\"softpl\", \"hardpl\", \"tent\", \"gce\"]\n",
    "reordercolumns = [\"noOOD\", \"CIFAR100-100\", \"CIFAR100-200\", \"CIFAR100C-100\", \"CIFAR100C-200\", \"SVHN-100\", \"SVHN-200\", \"SVHNC-100\", \"SVHNC-200\"] #[\"noOOD\", \"CIFAR100-200\", \"CIFAR100C-200\", \"SVHN-200\", \"SVHNC-200\"]\n",
    "\n",
    "# nur die beste lr\n",
    "df_1_2 = df_1_1.min(level= \"Adaptation\").reindex(reorderlist)\n",
    "df_1_2 = df_1_2[reordercolumns]\n",
    "\n",
    "\n",
    "df_1_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_2.to_csv(\"08_results_and_figures/res_100_200_all_lr_best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_1 = pd.DataFrame(dict_of_results).T\n",
    "df_1_1.index.names = [\"Adaptation\",\"lr\",\"PL_Threshold\",\"oodMethod\",\"oodThreshold\"]\n",
    "reorderlist = [\"source\", \"norm\",\"softpl\", \"hardpl\", \"tent\", \"gce\"]\n",
    "reordercolumns = [\"noOOD\", \"CIFAR100-100\", \"CIFAR100-200\", \"CIFAR100C-100\", \"CIFAR100C-200\", \"SVHN-100\", \"SVHN-200\", \"SVHNC-100\", \"SVHNC-200\"] #[\"noOOD\", \"CIFAR100-200\", \"CIFAR100C-200\", \"SVHN-200\", \"SVHNC-200\"]\n",
    "\n",
    "df_1_2 = df_1_1[reordercolumns]\n",
    "df_1_2 = df_1_2.reindex(reorderlist, level= \"Adaptation\")\n",
    "\n",
    "\n",
    "df_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_2.to_csv(\"08_results_and_figures/res_100_200_all_lr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_results = dict()\n",
    "for row in filtered_df_1[(\"ErrorRate\", \"min\")].index:\n",
    "    new_key = row[:5]\n",
    "    if new_key not in dict_of_results.keys():\n",
    "        dict_of_results[new_key] = dict()\n",
    "    used = False\n",
    "    for i in range(5,9):\n",
    "        if row[i] != 0:\n",
    "            dict_of_results[new_key][filtered_df_1.index.names[i]+\"-\"+str(row[i])] = [filtered_df_1[(\"ErrorRate\", \"min\")].loc[row], row[1], filtered_df_1[(\"ErrorRate\", \"minEpoch\")].loc[row][1]]\n",
    "            used=True\n",
    "    if used==False:\n",
    "        dict_of_results[new_key][\"noOOD\"] = [filtered_df_1[(\"ErrorRate\", \"min\")].loc[row], row[1], filtered_df_1[(\"ErrorRate\", \"minEpoch\")].loc[row][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_1 = pd.DataFrame(dict_of_results).T\n",
    "df_1_1.index.names = [\"Adaptation\",\"lr\",\"PL_Threshold\",\"oodMethod\",\"oodThreshold\"]\n",
    "reorderlist = [\"source\", \"norm\",\"softpl\", \"hardpl\", \"tent\", \"gce\"]\n",
    "reordercolumns = [\"noOOD\", \"CIFAR100-100\", \"CIFAR100-200\", \"CIFAR100C-100\", \"CIFAR100C-200\", \"SVHN-100\", \"SVHN-200\", \"SVHNC-100\", \"SVHNC-200\"] #[\"noOOD\", \"CIFAR100-200\", \"CIFAR100C-200\", \"SVHN-200\", \"SVHNC-200\"]\n",
    "\n",
    "df_1_2 = df_1_1.min(level= \"Adaptation\").reindex(reorderlist)\n",
    "df_1_2 = df_1_2[reordercolumns]\n",
    "\n",
    "\n",
    "df_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df_1_2.index:\n",
    "    print(row)\n",
    "    if row != \"source\":\n",
    "        for column in df_1_2.columns:\n",
    "            temp = df_1_2.loc[row,column]\n",
    "            print(\"'{}': [{},0.0,'none',0.0,{}],\".format(column, temp[1], temp[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standard corruptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "files.extend(get_output_files(output_dir=\"./04_output/output_leo/12*/*/\"))\n",
    "files.extend(get_output_files(output_dir=\"./04_output/output_leo/31*/standard/\"))\n",
    "results_df = generate_results_df_from_output_files(files)\n",
    "results_df[\"nOOD\"] = results_df[[\"CIFAR100\", \"CIFAR100C\", \"SVHN\", \"SVHNC\"]].sum(axis=1)\n",
    "agg_results = pd.pivot_table(results_df, values=[\"Error\"], index=['Adaptation', 'LearningRate', \"PL_Threshold\", \"OOD_DetMethod\", \"OOD_Threshold\", 'CIFAR100', 'CIFAR100C', 'SVHN', 'SVHNC', 'CorruptionType','nOOD'], columns=[\"Epoch\"], aggfunc={np.mean, \"count\"})[\"Error\"]\n",
    "agg_results = agg_results.rename(columns={\"mean\": \"ErrorRate\"})\n",
    "agg_results.ErrorRate = agg_results.ErrorRate.round(decimals=1)\n",
    "pd.set_option('display.max_rows', 195)\n",
    "agg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_results = dict()\n",
    "\n",
    "for row in agg_results.index:\n",
    "    adaptation = row[0]\n",
    "    lr = row[1]\n",
    "    ood_threshold = row[4]\n",
    "    temp_series = agg_results.loc[row]\n",
    "    score = temp_series[temp_series.notna()][-1]\n",
    "    new_key = row[0]\n",
    "    if new_key not in dict_of_results.keys():\n",
    "        dict_of_results[new_key] = dict()\n",
    "    used = False\n",
    "    for i in range(5,9):\n",
    "        if row[i] != 0:\n",
    "            dict_of_results[new_key][agg_results.index.names[i]+\"-\"+str(row[i])] = score\n",
    "            used=True\n",
    "    if used==False:\n",
    "        dict_of_results[new_key][\"noOOD\"] = score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_1 = pd.DataFrame(dict_of_results).T\n",
    "df_1_1.index.names = [\"Adaptation\"]#,\"lr\",\"PL_Threshold\",\"oodMethod\",\"oodThreshold\"]\n",
    "\n",
    "reorderlist = [\"source\", \"norm\",\"softpl\", \"hardpl\", \"tent\", \"gce\"]\n",
    "reordercolumns = [\"noOOD\", \"CIFAR100-100\", \"CIFAR100-200\", \"CIFAR100C-100\", \"CIFAR100C-200\", \"SVHN-100\", \"SVHN-200\", \"SVHNC-100\", \"SVHNC-200\"] #[\"noOOD\", \"CIFAR100-200\", \"CIFAR100C-200\", \"SVHN-200\", \"SVHNC-200\"]\n",
    "\n",
    "df_1_2 = df_1_1.min(level= \"Adaptation\").reindex(reorderlist)\n",
    "df_1_2 = df_1_2[reordercolumns]\n",
    "\n",
    "\n",
    "df_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_2.to_csv(\"08_results_and_figures/res_100_200_standard_best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table2: ood detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "files.extend(get_output_files(output_dir=\"./04_output/output_leo/10*/*/\"))\n",
    "files.extend(get_output_files(output_dir=\"./04_output/output_leo/31*/holdout/\"))\n",
    "results_df = generate_results_df_from_output_files(files)\n",
    "results_df[\"nOOD\"] = results_df[[\"CIFAR100\", \"CIFAR100C\", \"SVHN\", \"SVHNC\"]].sum(axis=1)\n",
    "agg_results = pd.pivot_table(results_df, values=[\"Error\"], index=['Adaptation', 'LearningRate', \"PL_Threshold\", \"OOD_DetMethod\", \"OOD_Threshold\", 'CIFAR100', 'CIFAR100C', 'SVHN', 'SVHNC', 'CorruptionType','nOOD'], columns=[\"Epoch\"], aggfunc={np.mean, \"count\"})[\"Error\"]\n",
    "agg_results = agg_results.rename(columns={\"mean\": \"ErrorRate\"})\n",
    "agg_results.ErrorRate = agg_results.ErrorRate.round(decimals=1)\n",
    "pd.set_option('display.max_rows', 195)\n",
    "agg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noods = [0,100]\n",
    "filtered_df_1 = agg_results.query(\"nOOD in @noods and PL_Threshold not in ['0.5','0.7']\")\n",
    "filtered_df_1[(\"ErrorRate\", \"min\")] = filtered_df_1[[(\"ErrorRate\", str(x))for x in range(1,7)]].min(axis=1)\n",
    "filtered_df_1[(\"ErrorRate\", \"minEpoch\")] = filtered_df_1[[(\"ErrorRate\", str(x))for x in range(1,7)]].idxmin(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_results = dict()\n",
    "for row in filtered_df_1[(\"ErrorRate\", \"min\")].index:\n",
    "    new_key = row[:5]\n",
    "    if new_key not in dict_of_results.keys():\n",
    "        dict_of_results[new_key] = dict()\n",
    "    used = False\n",
    "    for i in range(5,9):\n",
    "        if row[i] != 0:\n",
    "            dict_of_results[new_key][filtered_df_1.index.names[i]+\"-\"+str(row[i])] = filtered_df_1[(\"ErrorRate\", \"min\")].loc[row]\n",
    "            used=True\n",
    "    if used==False:\n",
    "        dict_of_results[new_key][\"noOOD\"] = filtered_df_1[(\"ErrorRate\", \"min\")].loc[row]\n",
    "dict_of_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_1 = pd.DataFrame(dict_of_results).T\n",
    "df_1_1.index.names = [\"Adaptation\",\"lr\",\"PL_Threshold\",\"oodMethod\",\"oodThreshold\"]\n",
    "\n",
    "reorderlist = [\"source\", \"norm\",\"softpl\", \"hardpl\", \"tent\", \"gce\"]#[\"source\", \"norm\",\"softpl\", \"hardpl\", \"tent\", \"gce\"]\n",
    "reordercolumns = [\"noOOD\",\"CIFAR100-100\", \"CIFAR100C-100\", \"SVHN-100\", \"SVHNC-100\"]#[\"noOOD\", \"CIFAR100-100\", \"CIFAR100-200\", \"CIFAR100C-100\", \"CIFAR100C-200\", \"SVHN-100\", \"SVHN-200\", \"SVHNC-100\", \"SVHNC-200\"] #[\"noOOD\", \"CIFAR100-200\", \"CIFAR100C-200\", \"SVHN-200\", \"SVHNC-200\"]\n",
    "\n",
    "# nur die beste lr\n",
    "df_1_2 = df_1_1.min(level= \"Adaptation\").reindex(reorderlist)\n",
    "df_1_2 = df_1_2[reordercolumns]\n",
    "\n",
    "\n",
    "df_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_2.to_csv(\"08_results_and_figures/res_100_ood_detection_best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### alle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_1 = pd.DataFrame(dict_of_results).T\n",
    "df_1_1.index.names = [\"Adaptation\",\"lr\",\"PL_Threshold\",\"oodMethod\",\"oodThreshold\"]\n",
    "\n",
    "reorderlist = [\"source\", \"norm\",\"softpl\", \"hardpl\", \"tent\", \"gce\"]\n",
    "reordercolumns = [\"CIFAR100-100\", \"CIFAR100C-100\", \"SVHN-100\", \"SVHNC-100\"]#[\"noOOD\", \"CIFAR100-100\", \"CIFAR100-200\", \"CIFAR100C-100\", \"CIFAR100C-200\", \"SVHN-100\", \"SVHN-200\", \"SVHNC-100\", \"SVHNC-200\"] #[\"noOOD\", \"CIFAR100-200\", \"CIFAR100C-200\", \"SVHN-200\", \"SVHNC-200\"]\n",
    "\n",
    "# alle\n",
    "\n",
    "df_1_2 = df_1_1[reordercolumns]\n",
    "df_1_2 = df_1_2.reindex(reorderlist, level= \"Adaptation\")\n",
    "\n",
    "\n",
    "df_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_2.to_csv(\"08_results_and_figures/res_100_ood_detection_all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_results = dict()\n",
    "for row in filtered_df_1[(\"ErrorRate\", \"min\")].index:\n",
    "    new_key = row[:5]\n",
    "    if new_key not in dict_of_results.keys():\n",
    "        dict_of_results[new_key] = dict()\n",
    "    used = False\n",
    "    for i in range(5,9):\n",
    "        if row[i] != 0:\n",
    "            dict_of_results[new_key][filtered_df_1.index.names[i]+\"-\"+str(row[i])] = [filtered_df_1[(\"ErrorRate\", \"min\")].loc[row], row[1], filtered_df_1[(\"ErrorRate\", \"minEpoch\")].loc[row][1], row[4]]\n",
    "            used=True\n",
    "    if used==False:\n",
    "        dict_of_results[new_key][\"noOOD\"] = [filtered_df_1[(\"ErrorRate\", \"min\")].loc[row], row[1], filtered_df_1[(\"ErrorRate\", \"minEpoch\")].loc[row][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_1 = pd.DataFrame(dict_of_results).T\n",
    "df_1_1.index.names = [\"Adaptation\",\"lr\",\"PL_Threshold\",\"oodMethod\",\"oodThreshold\"]\n",
    "\n",
    "reorderlist = [\"softpl\", \"hardpl\", \"tent\", \"gce\"]\n",
    "reordercolumns = [\"CIFAR100-100\", \"CIFAR100C-100\", \"SVHN-100\", \"SVHNC-100\"] #[\"noOOD\", \"CIFAR100-200\", \"CIFAR100C-200\", \"SVHN-200\", \"SVHNC-200\"]\n",
    "\n",
    "df_1_2 = df_1_1[reordercolumns]\n",
    "df_1_2 = df_1_2.reindex(reorderlist, level= \"Adaptation\")\n",
    "\n",
    "\n",
    "df_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df_1_2.index:\n",
    "    print(row)\n",
    "    if row != \"source\":\n",
    "        for column in df_1_2.columns:\n",
    "            \n",
    "            temp = df_1_2.loc[row,column]\n",
    "            if isinstance(temp,list):\n",
    "                print(\"'{}': [{},0.0,'none',0.0,{},{}],\".format(column, temp[1], temp[2],temp[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standard corruptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "files.extend(get_output_files(output_dir=\"./04_output/output_leo/13*/*/\"))\n",
    "files.extend(get_output_files(output_dir=\"./04_output/output_leo/31*/standard/\"))\n",
    "results_df = generate_results_df_from_output_files(files)\n",
    "results_df[\"nOOD\"] = results_df[[\"CIFAR100\", \"CIFAR100C\", \"SVHN\", \"SVHNC\"]].sum(axis=1)\n",
    "agg_results = pd.pivot_table(results_df, values=[\"Error\"], index=['Adaptation', 'LearningRate', \"PL_Threshold\", \"OOD_DetMethod\", \"OOD_Threshold\", 'CIFAR100', 'CIFAR100C', 'SVHN', 'SVHNC', 'CorruptionType','nOOD'], columns=[\"Epoch\"], aggfunc={np.mean, \"count\"})[\"Error\"]\n",
    "agg_results = agg_results.rename(columns={\"mean\": \"ErrorRate\"})\n",
    "agg_results.ErrorRate = agg_results.ErrorRate.round(decimals=1)\n",
    "pd.set_option('display.max_rows', 195)\n",
    "agg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_results = dict()\n",
    "\n",
    "for row in agg_results.index:\n",
    "    adaptation = row[0]\n",
    "    lr = row[1]\n",
    "    ood_threshold = row[4]\n",
    "    temp_series = agg_results.loc[row]\n",
    "    score = temp_series[temp_series.notna()][-1]\n",
    "    new_key = row[0]\n",
    "    if new_key not in dict_of_results.keys():\n",
    "        dict_of_results[new_key] = dict()\n",
    "    used = False\n",
    "    for i in range(5,9):\n",
    "        if row[i] != 0:\n",
    "            dict_of_results[new_key][agg_results.index.names[i]+\"-\"+str(row[i])] = score\n",
    "            used=True\n",
    "    if used==False:\n",
    "        dict_of_results[new_key][\"noOOD\"] = score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_1 = pd.DataFrame(dict_of_results).T\n",
    "df_1_1.index.names = [\"Adaptation\"]#,\"lr\",\"PL_Threshold\",\"oodMethod\",\"oodThreshold\"]\n",
    "\n",
    "reorderlist = [\"source\", \"norm\",\"softpl\", \"hardpl\", \"tent\", \"gce\"]\n",
    "reordercolumns = [\"noOOD\", \"CIFAR100-100\", \"CIFAR100C-100\", \"SVHN-100\", \"SVHNC-100\"] #[\"noOOD\", \"CIFAR100-200\", \"CIFAR100C-200\", \"SVHN-200\", \"SVHNC-200\"]\n",
    "\n",
    "df_1_2 = df_1_1.min(level= \"Adaptation\").reindex(reorderlist)\n",
    "df_1_2 = df_1_2[reordercolumns]\n",
    "\n",
    "\n",
    "df_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_2.to_csv(\"08_results_and_figures/res_100_ood_detection_standard_best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table3: thresholding w/o ood data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "files.extend(get_output_files(output_dir=\"./04_output/output_leo/11*/*/\"))\n",
    "results_df = generate_results_df_from_output_files(files)\n",
    "results_df[\"nOOD\"] = results_df[[\"CIFAR100\", \"CIFAR100C\", \"SVHN\", \"SVHNC\"]].sum(axis=1)\n",
    "agg_results = pd.pivot_table(results_df, values=[\"Error\"], index=['Adaptation', 'LearningRate', \"PL_Threshold\", \"OOD_DetMethod\", \"OOD_Threshold\", 'CIFAR100', 'CIFAR100C', 'SVHN', 'SVHNC', 'CorruptionType','nOOD'], columns=[\"Epoch\"], aggfunc={np.mean, \"count\"})[\"Error\"]\n",
    "agg_results = agg_results.rename(columns={\"mean\": \"ErrorRate\"})\n",
    "agg_results.ErrorRate = agg_results.ErrorRate.round(decimals=1)\n",
    "pd.set_option('display.max_rows', 195)\n",
    "agg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results[(\"ErrorRate\", \"min\")] = agg_results[[(\"ErrorRate\", str(x))for x in range(1,7)]].min(axis=1)\n",
    "agg_results[(\"ErrorRate\", \"minEpoch\")] = agg_results[[(\"ErrorRate\", str(x))for x in range(1,7)]].idxmin(axis=1)\n",
    "filtered_df_1 = agg_results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_results = dict()\n",
    "for row in filtered_df_1[(\"ErrorRate\", \"min\")].index:\n",
    "    \n",
    "    new_key = row[:5]\n",
    "    if new_key not in dict_of_results.keys():\n",
    "        dict_of_results[new_key] = dict()\n",
    "    used = False\n",
    "    for i in range(5,9):\n",
    "        if row[i] != 0:\n",
    "            dict_of_results[new_key][filtered_df_1.index.names[i]+\"-\"+str(row[i])] = filtered_df_1[(\"ErrorRate\", \"min\")].loc[row]\n",
    "            used=True\n",
    "    if used==False:\n",
    "        dict_of_results[new_key][\"noOOD\"] = filtered_df_1[(\"ErrorRate\", \"min\")].loc[row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_1 = pd.DataFrame(dict_of_results).T\n",
    "df_1_1.index.names = [\"Adaptation\",\"lr\",\"PL_Threshold\",\"oodMethod\",\"oodThreshold\"]\n",
    "\n",
    "reorderlist = [\"source\", \"norm\",\"softpl\", \"hardpl\", \"tent\", \"gce\"]\n",
    "reordercolumns = [\"noOOD\"]#[\"noOOD\", \"CIFAR100-100\", \"CIFAR100-200\", \"CIFAR100C-100\", \"CIFAR100C-200\", \"SVHN-100\", \"SVHN-200\", \"SVHNC-100\", \"SVHNC-200\"] #[\"noOOD\", \"CIFAR100-200\", \"CIFAR100C-200\", \"SVHN-200\", \"SVHNC-200\"]\n",
    "\n",
    "# alle\n",
    "\n",
    "df_1_2 = df_1_1[reordercolumns]\n",
    "df_1_2 = df_1_2.reindex(reorderlist, level= \"Adaptation\")\n",
    "\n",
    "\n",
    "df_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_2.to_csv(\"08_results_and_figures/res_thresholding_noood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_results = dict()\n",
    "for row in filtered_df_1[(\"ErrorRate\", \"min\")].index:\n",
    "    \n",
    "    new_key = row[:5]\n",
    "   \n",
    "    if new_key not in dict_of_results.keys():\n",
    "        dict_of_results[new_key] = dict()\n",
    "    used = False\n",
    "    for i in range(5,9):\n",
    "        if row[i] != 0:\n",
    "            dict_of_results[new_key][filtered_df_1.index.names[i]+\"-\"+str(row[i])] = [filtered_df_1[(\"ErrorRate\", \"min\")].loc[row], row[1], filtered_df_1[(\"ErrorRate\", \"minEpoch\")].loc[row][1]]\n",
    "            used=True\n",
    "    if used==False:\n",
    "        dict_of_results[new_key][\"noOOD\"] = [filtered_df_1[(\"ErrorRate\", \"min\")].loc[row], row[1], filtered_df_1[(\"ErrorRate\", \"minEpoch\")].loc[row][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_1 = pd.DataFrame(dict_of_results).T\n",
    "df_1_1.index.names = [\"Adaptation\",\"lr\",\"PL_Threshold\",\"oodMethod\",\"oodThreshold\"]\n",
    "\n",
    "reorderlist = [\"source\", \"norm\",\"softpl\", \"hardpl\", \"tent\", \"gce\"]\n",
    "reordercolumns = [\"noOOD\"] #[\"noOOD\", \"CIFAR100-200\", \"CIFAR100C-200\", \"SVHN-200\", \"SVHNC-200\"]\n",
    "\n",
    "df_1_2 = df_1_1[reordercolumns]\n",
    "df_1_2 = df_1_2.reindex(reorderlist, level= \"Adaptation\")\n",
    "\n",
    "\n",
    "df_1_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standard corruptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "files.extend(get_output_files(output_dir=\"./04_output/output_leo/14*/*/\"))\n",
    "results_df = generate_results_df_from_output_files(files)\n",
    "results_df[\"nOOD\"] = results_df[[\"CIFAR100\", \"CIFAR100C\", \"SVHN\", \"SVHNC\"]].sum(axis=1)\n",
    "agg_results = pd.pivot_table(results_df, values=[\"Error\"], index=['Adaptation', 'LearningRate', \"PL_Threshold\", \"OOD_DetMethod\", \"OOD_Threshold\", 'CIFAR100', 'CIFAR100C', 'SVHN', 'SVHNC', 'CorruptionType','nOOD'], columns=[\"Epoch\"], aggfunc={np.mean, \"count\"})[\"Error\"]\n",
    "agg_results = agg_results.rename(columns={\"mean\": \"ErrorRate\"})\n",
    "agg_results.ErrorRate = agg_results.ErrorRate.round(decimals=1)\n",
    "pd.set_option('display.max_rows', 195)\n",
    "agg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_results = dict()\n",
    "\n",
    "for row in agg_results.index:\n",
    "    adaptation = row[0]\n",
    "    lr = row[1]\n",
    "    ood_threshold = row[4]\n",
    "    temp_series = agg_results.loc[row]\n",
    "    score = temp_series[temp_series.notna()][-1]\n",
    "    new_key = row[0]\n",
    "    if new_key not in dict_of_results.keys():\n",
    "        dict_of_results[new_key] = dict()\n",
    "    used = False\n",
    "    for i in range(5,9):\n",
    "        if row[i] != 0:\n",
    "            dict_of_results[new_key][agg_results.index.names[i]+\"-\"+str(row[i])] = score\n",
    "            used=True\n",
    "    if used==False:\n",
    "        dict_of_results[new_key][\"noOOD\"] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_1 = pd.DataFrame(dict_of_results).T\n",
    "df_1_1.index.names = [\"Adaptation\"]#,\"lr\",\"PL_Threshold\",\"oodMethod\",\"oodThreshold\"]\n",
    "\n",
    "reorderlist = [\"softpl\", \"hardpl\", \"tent\", \"gce\"]\n",
    "reordercolumns = [\"noOOD\"] #[\"noOOD\", \"CIFAR100-200\", \"CIFAR100C-200\", \"SVHN-200\", \"SVHNC-200\"]\n",
    "\n",
    "df_1_2 = df_1_1.min(level= \"Adaptation\").reindex(reorderlist)\n",
    "df_1_2 = df_1_2[reordercolumns]\n",
    "\n",
    "df_1_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_2.to_csv(\"08_results_and_figures/res_thresholding_noood_standard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ablation on q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "files.extend(get_output_files(output_dir=\"./04_output/output_leo/31*/holdout/\"))\n",
    "files.extend(get_output_files(output_dir=\"./04_output/output_leo/08*/*/\"))\n",
    "\n",
    "results_df = generate_results_df_from_output_files(files)\n",
    "\n",
    "agg_results = pd.pivot_table(results_df, values=[\"Error\"], index=['Adaptation', 'LearningRate', \"PL_Threshold\", \"OOD_DetMethod\", \"OOD_Threshold\", 'CIFAR100', 'CIFAR100C', 'SVHN', 'SVHNC', 'CorruptionType'], columns=[\"Epoch\"], aggfunc={np.mean, \"count\"})[\"Error\"]\n",
    "agg_results = agg_results.rename(columns={\"mean\": \"ErrorRate\"}) #, \"count\": \"# of entries\"\n",
    "agg_results.ErrorRate = agg_results.ErrorRate.round(decimals=1)\n",
    "agg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_1 = agg_results.query(\"Adaptation=='gce' and CIFAR100 == 0\")\n",
    "filtered_df_1[(\"ErrorRate\", \"min\")] = filtered_df_1[[(\"ErrorRate\", str(x))for x in range(1,7)]].min(axis=1)\n",
    "filtered_df_1[(\"ErrorRate\", \"minEpoch\")] = filtered_df_1[[(\"ErrorRate\", str(x))for x in range(1,7)]].idxmin(axis=1)\n",
    "filtered_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_results = dict()\n",
    "for row in filtered_df_1[(\"ErrorRate\", \"min\")].index:\n",
    "    \n",
    "    new_key = row[:5]\n",
    "    \n",
    "    if new_key not in dict_of_results.keys():\n",
    "        dict_of_results[new_key] = dict()\n",
    "    used = False\n",
    "    for i in range(5,9):\n",
    "        if row[i] != 0:\n",
    "            dict_of_results[new_key][filtered_df_1.index.names[i]+\"-\"+str(row[i])] = filtered_df_1[(\"ErrorRate\", \"min\")].loc[row]\n",
    "            used=True\n",
    "    if used==False:\n",
    "        dict_of_results[new_key][\"noOOD\"] = filtered_df_1[(\"ErrorRate\", \"min\")].loc[row]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_1 = pd.DataFrame(dict_of_results).T\n",
    "df_1_1.index.names = [\"Adaptation\",\"lr\",\"PL_Threshold\",\"oodMethod\",\"oodThreshold\"]\n",
    "reordercolumns = [\"noOOD\",\"CIFAR100C-100\", \"CIFAR100C-200\", \"SVHN-100\", \"SVHN-200\"] #[\"noOOD\", \"CIFAR100-200\", \"CIFAR100C-200\", \"SVHN-200\", \"SVHNC-200\"]\n",
    "\n",
    "# nur die beste lr\n",
    "df_1_2 = df_1_1.min(level= \"PL_Threshold\")\n",
    "df_1_2 = df_1_2[reordercolumns]\n",
    "\n",
    "\n",
    "df_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_2.to_csv(\"08_results_and_figures/ablation_q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### which data is filtered out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "files.extend(get_output_files(output_dir=\"./04_output/output_leo/13*/*/\"))\n",
    "results_df = generate_thresholding_df_from_output_files(files)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results = pd.pivot_table(results_df, values=[\"Error\",\"FilteredTotal\", \"FilteredCifar\", \"FilteredOOD\"], index=['Adaptation', 'LearningRate', \"PL_Threshold\", \"OOD_DetMethod\", \"OOD_Threshold\", 'CIFAR100', 'CIFAR100C', 'SVHN', 'SVHNC', 'CorruptionType'], columns=[\"Epoch\"], aggfunc={np.mean})\n",
    "\n",
    "agg_results = agg_results.round(decimals=1)\n",
    "pd.set_option('display.max_rows', 195)\n",
    "agg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_results = dict()\n",
    "\n",
    "for row in agg_results.index:\n",
    "    adaptation = row[0]\n",
    "    lr = row[1]\n",
    "    ood_threshold = row[4]\n",
    "    temp_series = agg_results.loc[row]\n",
    "    \n",
    "    score = []\n",
    "    for i in [\"Error\", \"FilteredCifar\", \"FilteredOOD\"]:\n",
    "        \n",
    "        score.append(temp_series[(i,\"mean\")][temp_series[(i,\"mean\")].notna()][-1])\n",
    "    new_key = row[0]\n",
    "    if new_key not in dict_of_results.keys():\n",
    "        dict_of_results[new_key] = dict()\n",
    "    used = False\n",
    "    for i in range(5,9):\n",
    "        if row[i] != 0:\n",
    "            dict_of_results[new_key][agg_results.index.names[i]+\"-\"+str(row[i])] = score\n",
    "            used=True\n",
    "    if used==False:\n",
    "        dict_of_results[new_key][\"noOOD\"] = score\n",
    "\n",
    "dict_of_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_1 = pd.DataFrame(dict_of_results).T\n",
    "df_1_1.index.names = [\"Adaptation\"]#,\"lr\",\"PL_Threshold\",\"oodMethod\",\"oodThreshold\"]\n",
    "\n",
    "reorderlist = [\"softpl\", \"hardpl\", \"tent\", \"gce\"]\n",
    "reordercolumns = [\"CIFAR100-100\", \"CIFAR100C-100\", \"SVHN-100\", \"SVHNC-100\"] #[\"noOOD\", \"CIFAR100-200\", \"CIFAR100C-200\", \"SVHN-200\", \"SVHNC-200\"]\n",
    "\n",
    "df_1_2 = df_1_1.min(level= \"Adaptation\").reindex(reorderlist)\n",
    "df_1_2 = df_1_2[reordercolumns]\n",
    "\n",
    "df_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_2.to_csv(\"08_results_and_figures/res_thresholding_filtered_out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "files.extend(get_output_files(output_dir=\"./04_output/output_leo/09*/*/\"))\n",
    "files.extend(get_output_files(output_dir=\"./04_output/output_leo/10*/*/\"))\n",
    "results_df = generate_results_df_from_output_files(files)\n",
    "data = results_df.query(\"LearningRate not in ['0.003', '0.0003'] and CIFAR100==0 and CIFAR100C==0 and SVHNC==0\")\n",
    "data = data.drop_duplicates()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "\n",
    "for setting in [\"none\", \"threshold_n_images\"]:\n",
    "\n",
    "    filtered_data = data.query(\"Adaptation == @adaptation and OOD_DetMethod == @setting\")\n",
    "    source_data = data.query(\"Adaptation == 'source'\")\n",
    "    norm_data = data.query(\"Adaptation == 'norm'\")\n",
    "\n",
    "    data_list = [filtered_data, norm_data]#[filtered_data, source_data, norm_data]\n",
    "\n",
    "    epochs = range(1,7)\n",
    "    \n",
    "    for data_elem in data_list:\n",
    "        max = list()\n",
    "        min = list()\n",
    "        mean = list()\n",
    "        for epoch in epochs:\n",
    "            \n",
    "            mean.append(data_elem[data_elem[\"Epoch\"]==str(epoch)][\"Error\"].mean())\n",
    "            \n",
    "        ax.plot(epochs, mean, label=adaptation+'_'+setting)#label=\"PL_Threshold = {}\".format(condition_temp)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.set_xlabel(\"Epochs\")\n",
    "        ax.set_ylabel(\"Error Rate [%]\")    \n",
    "\n",
    "\n",
    "fig.subplots_adjust(wspace=0.35, hspace=0.35)\n",
    "fig.patch.set_facecolor('white')\n",
    "fig.legend(#title = \"[Adaptation, lr, Corruptions, OOD_samples, OOD_detection]\", \n",
    "    loc=\"upper center\", bbox_to_anchor=(0.5,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = filtered_df\n",
    "\n",
    "## accuracy of source on the corruptions\n",
    "source_performance_dict = {\n",
    "    \"holdout\": 23.42,\n",
    "    \"standard\": 26.54\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,5))\n",
    "\n",
    "plot_source_as_0 = False\n",
    "plot_source_as_hline = False\n",
    "plot_min_max_bands = True\n",
    "\n",
    "if plot_source_as_0:\n",
    "    n_epochs = len(data[\"ErrorRate\"].columns)\n",
    "    epochs = range(0,n_epochs+1)\n",
    "    x = epochs\n",
    "\n",
    "    for row in range(len(data)):\n",
    "        y = [source_performance_dict[data.iloc[row].name[9]]]\n",
    "        y.extend(data.iloc[row][\"ErrorRate\"].tolist())\n",
    "        ax.plot(x, y, label=transform_name_to_label(data.iloc[row].name))\n",
    "\n",
    "elif plot_source_as_hline:\n",
    "    n_epochs = len(data[\"ErrorRate\"].columns)\n",
    "    epochs = range(1,n_epochs+1)\n",
    "    x = epochs\n",
    "\n",
    "    for row in range(len(data)):\n",
    "        y = data.iloc[row][\"ErrorRate\"].tolist()\n",
    "        ax.hlines(y=source_performance_dict[data.iloc[row].name[9]],xmin=1, xmax=6)\n",
    "        ax.plot(x, y, label=transform_name_to_label(data.iloc[row].name))\n",
    "\n",
    "elif plot_min_max_bands:\n",
    "    n_epochs = 6#len(data[\"Error\"].columns)\n",
    "    epochs = range(1,n_epochs+1)\n",
    "    x = epochs\n",
    "    #print(data)\n",
    "    condition_temps = [\"none\",\"threshold_n_images\"]#[\"norm\",\"softpl\", \"harpl\",\"tent\",\"gce\"]#['0.7','0.8']\n",
    "    condition = \"OOD_DetMethod == @condition_temp\"#\"PL_Threshold == @condition_temp\"\n",
    "    for condition_temp in condition_temps:\n",
    "        data_part = data.query(condition)\n",
    "        #print(data_part)\n",
    "        max = data_part[\"Error\"].max().tolist()\n",
    "        min = data_part[\"Error\"].min().tolist()\n",
    "        mean = data_part[\"Error\"].mean().tolist()\n",
    "        (print(max,min,mean))\n",
    "        ax.plot(x, mean, label=\"PL_Threshold = {}\".format(condition_temp))\n",
    "        ax.fill_between(x, max, min, alpha=0.2)\n",
    "\n",
    "    fig.legend(#title = \"[Adaptation, lr, Corruptions, OOD_samples, OOD_detection]\", \n",
    "                loc=\"upper center\", bbox_to_anchor=(0.5,0))#len(data)*-0.04))\n",
    "\n",
    "labels = [str(epoch) for epoch in epochs]\n",
    "ax.set_xticks(ticks=x, labels = labels)\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "#ax.set_ylim(0,80)\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error Rate [%]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chart error rate per corruption and severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "files.extend(get_output_files(output_dir=\"./04_output/output_leo/31*/standard*/\"))\n",
    "files.extend(get_output_files(output_dir=\"./04_output/output_leo/31*/holdout*/\"))\n",
    "results_df = generate_results_df_from_output_files(files)\n",
    "agg_results = pd.pivot_table(results_df, values=[\"Error\"], index=[\"Adaptation\", \"Corruption\", \"Severity\",], columns=[\"Epoch\"], aggfunc={np.mean, \"count\"})[\"Error\"]\n",
    "agg_results = agg_results.rename(columns={\"mean\": \"ErrorRate\"})\n",
    "agg_results.ErrorRate = agg_results.ErrorRate.round(decimals=1)\n",
    "pd.set_option('display.max_rows', 195)\n",
    "agg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_1 = agg_results.query(\"Adaptation=='source'\")\n",
    "data = filtered_df_1[(\"ErrorRate\",\"1\")].loc[\"source\"]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = ['gaussian_noise', 'shot_noise', 'impulse_noise', 'speckle_noise']\n",
    "blur = ['defocus_blur', 'glass_blur', 'motion_blur', 'zoom_blur', 'gaussian_blur']\n",
    "weather =['snow', 'frost', 'fog', 'brightness', 'spatter']\n",
    "digital = ['contrast', 'elastic_transform', 'pixelate', 'jpeg_compression', 'saturate']\n",
    "labels = noise+blur+weather+digital\n",
    "\n",
    "noise1 = ['Gaussian noise', 'Shot noise', 'Impulse noise', '(Speckle noise)']\n",
    "blur1 = ['Defocus blur', 'Glass blur', 'Motion blur', 'Zoom blur', '(Gaussian blur)']\n",
    "weather1 =['Snow', 'Frost', 'Fog', 'Brightness', '(Spatter)']\n",
    "digital1 = ['Contrast', 'Elastic transform', 'Pixelate', 'Jpeg compression', '(Saturate)']\n",
    "labels_printable = noise1+blur1+weather1+digital1\n",
    "\n",
    "data = data.reindex(labels, level=\"Corruption\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"fog\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.arange(len(labels)) \n",
    "severities = range(1,6)\n",
    "width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,5))\n",
    "\n",
    "values_per_severity = {new_list: [] for new_list in severities}\n",
    "bars = list()\n",
    "for severity in severities:\n",
    "    for corruption in labels:\n",
    "        values_per_severity[severity].append(data[corruption][severity])\n",
    "\n",
    "    bar = plt.bar(ind+width*(severity-1), values_per_severity[severity], width, label=str(severity))\n",
    "  \n",
    "plt.xlabel(\"Corruptions\")\n",
    "plt.ylabel('Error Rate [%]')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "  \n",
    "plt.xticks(ind+width,labels_printable, rotation=45)\n",
    "plt.legend()\n",
    "plt.rc('font', size=20) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tsne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mvp\n",
    "file1 = \"97_embeddings/\"\n",
    "file2 = \"20220720_204411.npy\"\n",
    "abc = np.load(file1 +\"x_\"+ file2)\n",
    "xyz = np.load(file1 +\"y_\"+ file2)\n",
    "xyz = np.append(xyz, [10 for x in range(100)])\n",
    "print(abc.shape, xyz.shape)\n",
    "#abc = abc.reshape(abc.shape[0],640*64)\n",
    "#print(abc.shape, abc1.shape, xyz.shape)\n",
    "abc = abc.squeeze()\n",
    "print(abc.shape)\n",
    "\n",
    "# We want to get TSNE embedding with 2 dimensions\n",
    "n_components = 2\n",
    "tsne = TSNE(n_components)\n",
    "tsne_result = tsne.fit_transform(abc)\n",
    "tsne_result.shape\n",
    "\n",
    "tsne_result_df = pd.DataFrame({'tsne_1': tsne_result[:,0], 'tsne_2': tsne_result[:,1], 'label': xyz})\n",
    "fig, ax = plt.subplots(1)\n",
    "scatter = plt.scatter(x = tsne_result[:,0], y = tsne_result[:,1], c = xyz)\n",
    "#ax.legend(bbox_to_anchor=(1.05, 1))\n",
    "legend1 = ax.legend(*scatter.legend_elements(),loc=\"lower left\", title=\"Classes\")\n",
    "ax.add_artist(legend1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tsne(file1, file2):\n",
    "    x_embeddings = np.load(file1 +\"x_\"+ file2)\n",
    "    labels = np.load(file1 +\"y_\"+ file2)\n",
    "    #labels = np.append(labels, [10 for x in range(int(0.5*len(labels)))])\n",
    "    labels = np.concatenate((labels[0:200], [10 for x in range(100)], \n",
    "                                labels[200:400], [10 for x in range(100)], \n",
    "                                labels[400:600], [10 for x in range(100)], \n",
    "                                labels[600:800], [10 for x in range(100)], \n",
    "                                labels[800:], [10 for x in range(100)]))\n",
    "\n",
    "    x_embeddings_squeezed = x_embeddings.squeeze()\n",
    "    print(\"Input shape: Embeddings: {}, Squeezed Embeddings: {}, labels: {}\".format(x_embeddings.shape, x_embeddings_squeezed.shape, labels.shape))\n",
    "\n",
    "    # We want to get TSNE embedding with 2 dimensions\n",
    "    n_components = 2\n",
    "    tsne = TSNE(n_components)\n",
    "    tsne_result = tsne.fit_transform(x_embeddings_squeezed)\n",
    "    print(\"tsne result shape: \", tsne_result.shape)\n",
    "\n",
    "    return tsne_result, labels\n",
    "\n",
    "def plot_tsne(tsne_result_dict, labels_dict):\n",
    "    nrows = 3\n",
    "    ncols = 2\n",
    "    fig, axs = plt.subplots(nrows,ncols, figsize=(20,20))\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            scatter = axs[row][col].scatter(x = tsne_result_dict[(row,col)][:,0], y = tsne_result_dict[(row,col)][:,1], c = labels_dict[(row,col)])\n",
    "    legend1 = fig.legend(*scatter.legend_elements(),loc=\"lower left\", title=\"Classes\")\n",
    "    fig.add_artist(legend1)\n",
    "\n",
    "    cols = [\"saturate\", \"speckle_noise\"]\n",
    "    rows = ['Epoch {}'.format(row) for row in range(1,4)]\n",
    "\n",
    "    for ax, col in zip(axs[0], cols):\n",
    "        ax.set_title(col)\n",
    "\n",
    "    for ax, row in zip(axs[:,0], rows):\n",
    "        ax.set_ylabel(row, rotation=0, size='large')\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "def plot_tsne_1epoch(tsne_result_dict, labels_dict):\n",
    "    nrows = 1\n",
    "    ncols = 2\n",
    "    fig, axs = plt.subplots(nrows,ncols, figsize=(20,10), squeeze=False)\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            scatter = axs[row][col].scatter(x = tsne_result_dict[(row,col)][:,0], y = tsne_result_dict[(row,col)][:,1], c = labels_dict[(row,col)])\n",
    "    legend1 = fig.legend(*scatter.legend_elements(),loc=\"lower left\", title=\"Classes\")\n",
    "    fig.add_artist(legend1)\n",
    "\n",
    "    cols = [\"saturate\", \"speckle_noise\"]\n",
    "    rows = ['Epoch {}'.format(row) for row in range(1,4)]\n",
    "\n",
    "    for ax, col in zip(axs[0], cols):\n",
    "        ax.set_title(col)\n",
    "\n",
    "    for ax, row in zip(axs[:,0], rows):\n",
    "        ax.set_ylabel(row, rotation=0, size='large')\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "file1 = \"97_embeddings/tent_3epochs/\"\n",
    "tsne_result_dict = dict()\n",
    "labels_dict = dict()\n",
    "\n",
    "file2 = \"tent_CIFAR100_100_22-07-29_115654_487096_satu.txt_115709.npy\"\n",
    "pos = (0,0)\n",
    "tsne_result_dict[pos], labels_dict[pos] = get_tsne(file1, file2)\n",
    "\n",
    "file2 = \"tent_CIFAR100_100_22-07-29_115654_560074_spec.txt_115711.npy\"\n",
    "pos = (0,1)\n",
    "tsne_result_dict[pos], labels_dict[pos] = get_tsne(file1, file2)\n",
    "\n",
    "file2 = \"tent_CIFAR100_100_22-07-29_115654_487096_satu.txt_115736.npy\"\n",
    "pos = (1,0)\n",
    "tsne_result_dict[pos], labels_dict[pos] = get_tsne(file1, file2)\n",
    "\n",
    "file2 = \"tent_CIFAR100_100_22-07-29_115654_560074_spec.txt_115737.npy\"\n",
    "pos = (1,1)\n",
    "tsne_result_dict[pos], labels_dict[pos] = get_tsne(file1, file2)\n",
    "\n",
    "file2 = \"tent_CIFAR100_100_22-07-29_115654_487096_satu.txt_115803.npy\"\n",
    "pos = (2,0)\n",
    "tsne_result_dict[pos], labels_dict[pos] = get_tsne(file1, file2)\n",
    "\n",
    "file2 = \"tent_CIFAR100_100_22-07-29_115654_560074_spec.txt_115804.npy\"\n",
    "pos = (2,1)\n",
    "tsne_result_dict[pos], labels_dict[pos] = get_tsne(file1, file2)\n",
    "\n",
    "plot_tsne(tsne_result_dict, labels_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_1epoch(tsne_result_dict, labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = \"97_embeddings/tent_3epochs/\"\n",
    "tsne_result_dict = dict()\n",
    "labels_dict = dict()\n",
    "\n",
    "file2 = \"tent_SVHNC_100_22-07-29_115654_842551_satu.txt_115716.npy\"\n",
    "pos = (0,0)\n",
    "tsne_result_dict[pos], labels_dict[pos] = get_tsne(file1, file2)\n",
    "\n",
    "file2 = \"tent_SVHNC_100_22-07-29_115654_122570_spec.txt_115714.npy\"\n",
    "pos = (0,1)\n",
    "tsne_result_dict[pos], labels_dict[pos] = get_tsne(file1, file2)\n",
    "\n",
    "file2 = \"tent_SVHNC_100_22-07-29_115654_842551_satu.txt_115959.npy\"\n",
    "pos = (1,0)\n",
    "tsne_result_dict[pos], labels_dict[pos] = get_tsne(file1, file2)\n",
    "\n",
    "file2 = \"tent_SVHNC_100_22-07-29_115654_122570_spec.txt_120001.npy\"\n",
    "pos = (1,1)\n",
    "tsne_result_dict[pos], labels_dict[pos] = get_tsne(file1, file2)\n",
    "\n",
    "file2 = \"tent_SVHNC_100_22-07-29_115654_842551_satu.txt_120242.npy\"\n",
    "pos = (2,0)\n",
    "tsne_result_dict[pos], labels_dict[pos] = get_tsne(file1, file2)\n",
    "\n",
    "file2 = \"tent_SVHNC_100_22-07-29_115654_122570_spec.txt_120248.npy\"\n",
    "pos = (2,1)\n",
    "tsne_result_dict[pos], labels_dict[pos] = get_tsne(file1, file2)\n",
    "\n",
    "plot_tsne(tsne_result_dict, labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_1epoch(tsne_result_dict, labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = \"97_embeddings/tent_1epoch/\"\n",
    "file2 = \"tent_CIFAR100_100_22-07-29_010433_326820_spec.txt_010517.npy\"\n",
    "\n",
    "get_tsne(file1, file2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = \"97_embeddings/tent_1epoch/\"\n",
    "file2 = \"tent_CIFAR100C_100_22-07-29_010432_778817_satu.txt_010728.npy\"\n",
    "\n",
    "get_tsne(file1, file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = \"97_embeddings/tent_1epoch/\"\n",
    "file2 = \"tent_SVHN_100_22-07-29_010434_613801_satu.txt_010520.npy\"\n",
    "\n",
    "get_tsne(file1, file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = \"97_embeddings/tent_1epoch/\"\n",
    "file2 = \"tent_SVHNC_100_22-07-29_010434_479099_satu.txt_010740.npy\"\n",
    "\n",
    "get_tsne(file1, file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More visualization: npy to png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.utils import save_image\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "\n",
    "def get_10_samples_per_label(dataset, classes, path, nImages=10, labels=range(100), string_dataset=\"cifar10\"):\n",
    "\n",
    "    nClasses = len(classes)\n",
    "    fig, axs = plt.subplots(nClasses,nImages,figsize=(nImages*2, nClasses*2)) \n",
    "    counter = [0 for x in range(nClasses)]\n",
    "    print(counter)\n",
    "    \n",
    "    total_images_searched = 15*len(labels)\n",
    "\n",
    "    for i in range(total_images_searched):\n",
    "        for j in classes:\n",
    "            idx_j = classes.index(j)\n",
    "            img, label = dataset[i]\n",
    "            if label == j and counter[idx_j] < nImages:\n",
    "                #print(j, counter[j])\n",
    "                if string_dataset==\"svhn\":\n",
    "                    axs[idx_j, counter[idx_j]].imshow(img.permute(1,2,0))\n",
    "                    save_image(img, \"{}/{}_{}.jpg\".format(path, labels[j], counter[idx_j]))\n",
    "                else:\n",
    "                    axs[idx_j, counter[idx_j]].imshow(img)\n",
    "                    img.save(\"{}/{}_{}.jpg\".format(path, labels[j], counter[idx_j]))\n",
    "                axs[idx_j, counter[idx_j]].axis(\"off\")\n",
    "                #save_image(img, \"img{}_{}.png\".format(j,counter[j]))\n",
    "                \n",
    "                counter[idx_j] += 1\n",
    "\n",
    "    \n",
    "    selected_labels = [labels[x] for x in classes]\n",
    "    for ax, row in zip(axs[:,0], selected_labels):\n",
    "        if string_dataset==\"svhn\":\n",
    "            ax.annotate(row, xy=(0, 0), xytext=(-0.25, 0.5), size='xx-large',  xycoords=\"axes fraction\")\n",
    "        elif string_dataset==\"cifar100\":\n",
    "            ax.annotate(row, xy=(0, 0), xytext=(-1.25, 0.5), size='xx-large',  xycoords=\"axes fraction\")\n",
    "        else:\n",
    "            ax.annotate(row, xy=(0, 0), xytext=(-1, 0.5), size='xx-large',  xycoords=\"axes fraction\")\n",
    "    \n",
    "    fig.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.show()\n",
    "    print(counter)\n",
    "\n",
    "def get_corrupted_samples(path, corruptions, classes, labels_txt, corruption_labels, severity=3, svhn=False):\n",
    "\n",
    "    labels = np.load(path+\"/labels.npy\")\n",
    "\n",
    "    fig, axs = plt.subplots(len(classes),len(corruptions),figsize=(len(corruptions)*2, len(classes)*2))\n",
    "\n",
    "    for i, c in enumerate(corruptions):\n",
    "        dataset = np.load(path+\"/\"+c+\".npy\")\n",
    "        for j, l in enumerate(classes):\n",
    "            idx = np.where(labels==l)[0][0]\n",
    "        \n",
    "            \n",
    "            if svhn==True:\n",
    "                img = dataset[severity*26032 + idx]\n",
    "                img = np.reshape(img, (32,32,3))\n",
    "            else:\n",
    "                img = dataset[severity*10000 + idx]\n",
    "            axs[j, i].imshow(img)\n",
    "            axs[j, i].axis(\"off\")\n",
    "           \n",
    "    selected_labels = [labels_txt[x] for x in classes]\n",
    "    for ax, row in zip(axs[:,0], selected_labels):\n",
    "        if svhn:\n",
    "            ax.annotate(row, xy=(0, 0), xytext=(-0.25, 0.5), size='xx-large',  xycoords=\"axes fraction\")\n",
    "        else:\n",
    "            ax.annotate(row, xy=(0, 0), xytext=(-1, 0.5), size='xx-large',  xycoords=\"axes fraction\")\n",
    "\n",
    "    for ax, col in zip(axs[0], corruption_labels):\n",
    "        ax.annotate(col, xy=(0.5, 1), xytext=(0.5, 1.1), size='xx-large',  xycoords=\"axes fraction\", ha=\"center\",rotation=45)\n",
    "    \n",
    "    fig.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def get_severity_samples(path, corruptions, corruption_labels, severities=[1,2,3,4,5]):\n",
    "\n",
    "    labels = np.load(path+\"/labels.npy\")\n",
    "\n",
    "    fig, axs = plt.subplots(len(severities),len(corruptions),figsize=(len(corruptions)*2, len(severities)*2))\n",
    "\n",
    "    for i, c in enumerate(corruptions):\n",
    "        dataset = np.load(path+\"/\"+c+\".npy\")\n",
    "        for j, l in enumerate(severities):\n",
    "            idx = np.where(labels==5)[0][1]\n",
    "            \n",
    "            img = dataset[(l-1)*10000 + idx]\n",
    "            axs[j, i].imshow(img)\n",
    "            axs[j, i].axis(\"off\")\n",
    "           \n",
    "    selected_labels = severities\n",
    "    for ax, row in zip(axs[:,0], selected_labels):\n",
    "        ax.annotate(row, xy=(0, 0), xytext=(-0.25, 0.5), size='xx-large',  xycoords=\"axes fraction\")\n",
    "\n",
    "    for ax, col in zip(axs[0], corruption_labels):\n",
    "        ax.annotate(col, xy=(0.5, 1), xytext=(0.5, 1.1), size='xx-large',  xycoords=\"axes fraction\", ha=\"center\")\n",
    "    \n",
    "    fig.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"02_data/CIFAR-10\"\n",
    "cifar10 = datasets.CIFAR10(path, download=True, train=False)\n",
    "labels = [\"Airplane\", \"Automobile\", \"Bird\", \"Cat\", \"Deer\", \"Dog\", \"Frog\", \"Horse\", \"Ship\", \"Truck\"]\n",
    "\n",
    "get_10_samples_per_label(cifar10, classes=range(10), path=path, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cifar10c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"02_data/CIFAR-10-C/CIFAR-10-C/\"\n",
    "noise = ['gaussian_noise', 'shot_noise', 'impulse_noise', 'speckle_noise']\n",
    "blur = ['defocus_blur', 'glass_blur', 'motion_blur', 'zoom_blur', 'gaussian_blur']\n",
    "weather =['snow', 'frost', 'fog', 'brightness', 'spatter']\n",
    "digital = ['contrast', 'elastic_transform', 'pixelate', 'jpeg_compression', 'saturate']\n",
    "labels = noise+blur+weather+digital\n",
    "\n",
    "noise1 = ['Gaussian noise', 'Shot noise', 'Impulse noise', '(Speckle noise)']\n",
    "blur1 = ['Defocus blur', 'Glass blur', 'Motion blur', 'Zoom blur', '(Gaussian blur)']\n",
    "weather1 =['Snow', 'Frost', 'Fog', 'Brightness', '(Spatter)']\n",
    "digital1 = ['Contrast', 'Elastic transform', 'Pixelate', 'Jpeg compression', '(Saturate)']\n",
    "labels1 = noise1+blur1+weather1+digital1\n",
    "\n",
    "corruptions = labels#[\"gaussian_blur\", \"saturate\", \"spatter\", \"speckle_noise\"]\n",
    "corruption_labels = labels1#[\"Gaussian blur\", \"Saturate\", \"Spatter\", \"Speckle noise\"]\n",
    "classes = range(10)#[0,2,5,9]#range(10) #[2,3,7,9]\n",
    "labels = [\"Airplane\", \"Automobile\", \"Bird\", \"Cat\", \"Deer\", \"Dog\", \"Frog\", \"Horse\", \"Ship\", \"Truck\"]\n",
    "\n",
    "\n",
    "get_corrupted_samples(path, corruptions, classes, labels, corruption_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"02_data/CIFAR-10-C/\"\n",
    "corruptions = [\"gaussian_blur\", \"saturate\", \"spatter\", \"speckle_noise\"]\n",
    "corruption_labels = [\"Gaussian blur\", \"Saturate\", \"Spatter\", \"Speckle noise\"]\n",
    "\n",
    "get_severity_samples(path, corruptions, corruption_labels, severities=[1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cifar100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"02_data/CIFAR-100\"\n",
    "\n",
    "labels = [\"Apple\", \"Aquarium Fish\", \"Baby\", \"Bear\", \"Beaver\", \"Bed\", \"Bee\", \"Beetle\", \"Bicycle\", \"Bottle\", \"Bowl\", \"Boy\", \"Bridge\", \"Bus\", \"Butterfly\", \"Camel\", \"Can\", \"Castle\", \"Caterpillar\", \n",
    "            \"Cattle\", \"Chair\", \"Chimpanzee\", \"Clock\", \"Cloud\", \"Cockroach\", \"Couch\", \"Cra\", \"Crocodile\", \"Cup\", \"Dinosaur\", \"Dolphin\", \"Elephant\", \"Flatfish\", \"Forest\", \"Fox\", \"Girl\", \"Gamster\", \n",
    "            \"House\", \"Kangaroo\", \"Keyboard\", \"Lamp\", \"Lawn Mower\", \"Leopard\", \"Lion\", \"Lizard\", \"Lobster\", \"Man\", \"Maple Tree\", \"Motorcycle\", \"Mountain\", \"Mouse\", \"Mushroom\", \"Oak Tree\", \"Orange\", \n",
    "            \"Orchid\", \"Otter\", \"Palm Tree\", \"Pear\", \"Pickup Truck\", \"Pine Tree\", \"Plain\", \"Plate\", \"Poppy\", \"Porcupine\", \"Possum\", \"Rabbit\", \"Raccoon\", \"Ray\", \"Road\", \"Rocket\", \"Rose\", \"Sea\", \"Seal\", \n",
    "            \"Shark\", \"Shrew\", \"Skunk\", \"Skyscraper\", \"Snail\", \"Snake\", \"Spider\", \"Squirrel\", \"Streetcar\", \"Sunflower\", \"Sweet Pepper\", \"Table\", \"Tank\", \"Telephone\", \"Television\", \"Tiger\", \"Tractor\", \n",
    "            \"Train\", \"Trout\", \"Tulip\", \"Turtle\", \"Wardrobe\", \"Whale\", \"Willow Tree\", \"Wolf\", \"Woman\", \"Worm\"]\n",
    "labels.extend(range(10,100))\n",
    "classes = range(80,100)#[0,3,49,99] #range(100)\n",
    "\n",
    "cifar100 = datasets.CIFAR100(path, train=False, download=True)\n",
    "\n",
    "get_10_samples_per_label(cifar100, classes=classes, path=path, labels=labels,nImages=3, string_dataset=\"cifar100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cifar100c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"02_data/CIFAR-100-C\"\n",
    "corruptions = [\"gaussian_blur\", \"saturate\", \"spatter\", \"speckle_noise\"]\n",
    "corruption_labels = [\"Gaussian blur\", \"Saturate\", \"Spatter\", \"Speckle noise\"]\n",
    "classes = [0,3,49,99] #[0,3,8,9]#range(10) #[2,3,7,9]\n",
    "labels = [\"Apple\", \"Aquarium Fish\", \"Baby\", \"Bear\", \"Beaver\", \"Bed\", \"Bee\", \"Beetle\", \"Bicycle\", \"Bottle\", \"Bowl\", \"Boy\", \"Bridge\", \"Bus\", \"Butterfly\", \"Camel\", \"Can\", \"Castle\", \"Caterpillar\", \n",
    "            \"Cattle\", \"Chair\", \"Chimpanzee\", \"Clock\", \"Cloud\", \"Cockroach\", \"Couch\", \"Cra\", \"Crocodile\", \"Cup\", \"Dinosaur\", \"Dolphin\", \"Elephant\", \"Flatfish\", \"Forest\", \"Fox\", \"Girl\", \"Gamster\", \n",
    "            \"House\", \"Kangaroo\", \"Keyboard\", \"Lamp\", \"Lawn Mower\", \"Leopard\", \"Lion\", \"Lizard\", \"Lobster\", \"Man\", \"Maple Tree\", \"Motorcycle\", \"Mountain\", \"Mouse\", \"Mushroom\", \"Oak Tree\", \"Orange\", \n",
    "            \"Orchid\", \"Otter\", \"Palm Tree\", \"Pear\", \"Pickup Truck\", \"Pine Tree\", \"Plain\", \"Plate\", \"Poppy\", \"Porcupine\", \"Possum\", \"Rabbit\", \"Raccoon\", \"Ray\", \"Road\", \"Rocket\", \"Rose\", \"Sea\", \"Seal\", \n",
    "            \"Shark\", \"Shrew\", \"Skunk\", \"Skyscraper\", \"Snail\", \"Snake\", \"Spider\", \"Squirrel\", \"Streetcar\", \"Sunflower\", \"Sweet Pepper\", \"Table\", \"Tank\", \"Telephone\", \"Television\", \"Tiger\", \"Tractor\", \n",
    "            \"Train\", \"Trout\", \"Tulip\", \"Turtle\", \"Wardrobe\", \"Whale\", \"Willow Tree\", \"Wolf\", \"Woman\", \"Worm\"]\n",
    "\n",
    "\n",
    "get_corrupted_samples(path, corruptions, classes, labels, corruption_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### svhn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"02_data/SVHN\"\n",
    "svhn = datasets.SVHN(path, split=\"test\", transform=transforms.ToTensor(), download=True)\n",
    "classes = range(10)#[0,2,5,9] #range(10) #[0,1,5,9]\n",
    "get_10_samples_per_label(dataset=svhn, classes=classes, path=path, string_dataset=\"svhn\", nImages=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### svhnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"02_data/SVHN-C\"\n",
    "x_temp = np.load(path +\"/saturate.npy\")\n",
    "labels = np.load(path+\"/labels.npy\")\n",
    "img = x_temp[0]\n",
    "img = np.reshape(img, (32,32,3))\n",
    "plt.imshow(img)\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.load(\"02_data/SVHN-C\"+\"/labels.npy\")\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"02_data/SVHN-C\"\n",
    "corruptions = [\"gaussian_blur\", \"saturate\", \"spatter\", \"speckle_noise\"]\n",
    "corruption_labels = [\"Gaussian blur\", \"Saturate\", \"Spatter\", \"Speckle noise\"]\n",
    "classes = [0,2,5,9]#range(10) #[0,2,5,9]#range(10) #[0,1,5,9]#[2,3,7,9]\n",
    "\n",
    "get_corrupted_samples(path=path, corruptions=corruptions, classes=classes, labels_txt=range(10), corruption_labels=corruption_labels, svhn=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "466e6bf0790f4078eb62114584b71e8f519980680e8467648596ca25ed87cf35"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ba_env1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
